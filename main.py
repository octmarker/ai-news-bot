#!/usr/bin/env python3
"""AI News Bot - Google Cloud Functions ç‰ˆ
ãƒãƒ«ãƒã‚«ãƒ†ã‚´ãƒªãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆAIã€æ”¿æ²»çµŒæ¸ˆã€è«–æ–‡ã€ã‚»ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ”ãƒ†ã‚£ï¼‰ã‚’GitHubã«ã‚³ãƒŸãƒƒãƒˆ
"""

import os
import json
import re as _re
import functions_framework
from datetime import datetime, timezone, timedelta
from typing import Optional, Tuple, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor, as_completed

import trafilatura

# Import GNews client
from gnews_client import collect_multi_category_articles

from google import genai
from google.genai import types
from github import Github, Auth
from github.GithubException import GithubException


# ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®š
JST = timezone(timedelta(hours=9))


def get_jst_now() -> datetime:
    """ç¾åœ¨ã®JSTæ™‚åˆ»ã‚’å–å¾—"""
    return datetime.now(JST)


def log(message: str):
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°å‡ºåŠ›"""
    now = get_jst_now()
    print(f"[{now.strftime('%Y-%m-%d %H:%M:%S JST')}] {message}")


def get_ai_prompt(today: str, yesterday: str) -> str:
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
    return f"""ä»Šæ—¥ã¯{today}ã§ã™ã€‚ã‚ãªãŸã¯AIé–‹ç™ºãƒ„ãƒ¼ãƒ«å°‚é–€ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚

ã€é‡è¦ãªåˆ¶ç´„ã€‘
- **æ—¥æœ¬ã¨ã‚¢ãƒ¡ãƒªã‚«ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã¦ãã ã•ã„**ï¼ˆæ—¥æœ¬èªã¾ãŸã¯è‹±èªã®è¨˜äº‹ï¼‰
- **ä¸­å›½èªï¼ˆç°¡ä½“å­—ãƒ»ç¹ä½“å­—ï¼‰ã®ã‚½ãƒ¼ã‚¹ã¯çµ¶å¯¾ã«é™¤å¤–ã—ã¦ãã ã•ã„**
- **å°æ¹¾ã€é¦™æ¸¯ã€ä¸­å›½æœ¬åœŸã®ãƒ¡ãƒ‡ã‚£ã‚¢ã¯é™¤å¤–**ï¼ˆä¾‹ï¼šæ•¸ä½æ™‚ä»£ã€å·¥å•†æ™‚å ±ã€æ€å¦ã€ç¡¬æ˜¯è¦å­¸ãªã©ï¼‰
- å¿…ãš{yesterday}ã€œ{today}ã«å…¬é–‹ã•ã‚ŒãŸè¨˜äº‹ã®ã¿ã‚’å«ã‚ã¦ãã ã•ã„
- è¨˜äº‹ã®URLã‚„æœ¬æ–‡ã«å«ã¾ã‚Œã‚‹æ—¥ä»˜ã‚’ç¢ºèªã—ã€ãã‚Œä»¥å‰ã®å¤ã„è¨˜äº‹ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„
- è©²å½“æœŸé–“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ç„¡ç†ã«å¤ã„è¨˜äº‹ã‚’å«ã‚ãšã€Œè©²å½“ãªã—ã€ã¨ã—ã¦ãã ã•ã„

ã€åé›†å¯¾è±¡ã€‘
ä»¥ä¸‹ã«é–¢ã™ã‚‹**æŠ€è¡“çš„ãªãƒªãƒªãƒ¼ã‚¹ãƒ»ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæƒ…å ±**ã‚’åé›†ã—ã¦ãã ã•ã„ï¼š
- Google Antigravityï¼ˆæ–°æ©Ÿèƒ½ã€Skillsã€ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆï¼‰
- Claude Code / Claudeï¼ˆæ–°æ©Ÿèƒ½ã€MCPã€APIå¤‰æ›´ï¼‰
- Gemini / Google AIï¼ˆæ–°ãƒ¢ãƒ‡ãƒ«ã€æ–°æ©Ÿèƒ½ã€SDKæ›´æ–°ï¼‰
- ChatGPT / OpenAIï¼ˆæ–°æ©Ÿèƒ½ã€GPTsã€APIæ›´æ–°ï¼‰
- ãã®ä»–AIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ï¼ˆCursorã€GitHub Copilotã€Windsurfç­‰ï¼‰

ã€å„ªå…ˆã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
âœ… æ–°æ©Ÿèƒ½ãƒ»æ–°ã‚¹ã‚­ãƒ«ã®ãƒªãƒªãƒ¼ã‚¹ç™ºè¡¨
âœ… ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¢ãƒƒãƒ—ãƒ»ãƒã‚§ãƒ³ã‚¸ãƒ­ã‚°
âœ… æ–°APIãƒ»SDKãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å…¬é–‹
âœ… å…¬å¼ãƒ–ãƒ­ã‚°ãƒ»GitHubãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ
âœ… é–‹ç™ºè€…å‘ã‘ã®æŠ€è¡“è§£èª¬è¨˜äº‹

ã€é™¤å¤–ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
âŒ çµŒå–¶æˆ¦ç•¥ãƒ»è³‡é‡‘èª¿é”ãƒ»äººäº‹ãƒ‹ãƒ¥ãƒ¼ã‚¹
âŒ AIãƒªã‚¹ã‚¯ãƒ»è¦åˆ¶ãƒ»å€«ç†ã«é–¢ã™ã‚‹æ„è¦‹è¨˜äº‹
âŒ ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã®ä½¿ã„æ–¹ã‚¬ã‚¤ãƒ‰
âŒ å™‚ãƒ»ãƒªãƒ¼ã‚¯æƒ…å ±

ã€å‡ºåŠ›è¦ä»¶ã€‘
- æ—¥æœ¬ã¨ã‚¢ãƒ¡ãƒªã‚«ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã™ã‚‹
- åˆè¨ˆ5ã€œ7ä»¶ç¨‹åº¦ã«å³é¸
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¯**ä½•ãŒæ–°ã—ããªã£ãŸã‹**ã‚’å…·ä½“çš„ã«è¨˜è¼‰
- å‡ºåŠ›ã¯æ—¥æœ¬èªã§çµ±ä¸€

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
---
## [ãƒ„ãƒ¼ãƒ«/ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å]

### [ãƒªãƒªãƒ¼ã‚¹ãƒ»ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆå†…å®¹]
å…¬é–‹æ—¥: YYYY-MM-DD
[ä½•ãŒæ–°ã—ããªã£ãŸã‹ã‚’1ã€œ2è¡Œã§å…·ä½“çš„ã«èª¬æ˜]
â†’ [ã‚½ãƒ¼ã‚¹URL]

---

è©²å½“æœŸé–“ã«æŠ€è¡“çš„ãªãƒªãƒªãƒ¼ã‚¹æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæœ¬æ—¥ã®ä¸»è¦ãªãƒªãƒªãƒ¼ã‚¹æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"""


def get_ai_candidate_prompt(
    today: str, yesterday: str,
    boosted_keywords: list = None, suppressed_keywords: list = None,
    preferred_sources: list = None, category_distribution: dict = None,
    serendipity_ratio: float = 0.0, learning_phase: int = 0
) -> str:
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹å€™è£œç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ç‰ˆï¼‰"""
    boost_section = ""
    if boosted_keywords:
        boost_section = f"\nğŸ”¥ **ç‰¹ã«å„ªå…ˆ**: {', '.join(boosted_keywords)}"

    suppress_section = ""
    if suppressed_keywords:
        suppress_section = f"\nâ¬‡ï¸ **å„ªå…ˆåº¦ä¸‹ã’ã‚‹**: {', '.join(suppressed_keywords)}"

    # Phase 2+: ä¿¡é ¼ã‚½ãƒ¼ã‚¹ã¨ã‚«ãƒ†ã‚´ãƒªé…åˆ†
    source_section = ""
    if learning_phase >= 2 and preferred_sources:
        source_section = f"\nğŸ“° **ä¿¡é ¼ã™ã‚‹ã‚½ãƒ¼ã‚¹**: {', '.join(preferred_sources)}"

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥æœ€ä½ä»¶æ•°ã®ä¿è¨¼ï¼ˆPhase 2+ï¼‰
    category_section = ""
    if learning_phase >= 2 and category_distribution:
        cat_labels = {
            "ai": "AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
            "finance": "é‡‘èãƒ»çµŒæ¸ˆ",
            "politics": "æ”¿æ²»ãƒ»å¤–äº¤",
            "other": "ãã®ä»–"
        }
        # æœ€ä½ä»¶æ•°ã‚’æ˜ç¤º
        category_section = """

ã€é‡è¦ï¼šã‚«ãƒ†ã‚´ãƒªåˆ¥æœ€ä½ä»¶æ•°ã®ä¿è¨¼ã€‘
ä»¥ä¸‹ã®æœ€ä½ä»¶æ•°ã‚’å¿…ãšç¢ºä¿ã—ã¦ãã ã•ã„ã€‚ãƒ–ãƒ¼ã‚¹ãƒˆãƒ¯ãƒ¼ãƒ‰ã¯å„ã‚«ãƒ†ã‚´ãƒªå†…ã§ã®å„ªå…ˆé †ä½ä»˜ã‘ã«ä½¿ç”¨ã—ã¾ã™ï¼š
- AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼: æœ€ä½5ã€œ6ä»¶ï¼ˆAIãƒ„ãƒ¼ãƒ«ã€LLMã€æ©Ÿæ¢°å­¦ç¿’é–¢é€£ï¼‰
- é‡‘èãƒ»çµŒæ¸ˆ: æœ€ä½4ã€œ5ä»¶ï¼ˆæ—¥éŠ€ã€ç‚ºæ›¿ã€é‡‘èæ”¿ç­–ãªã©ï¼‰
- æ”¿æ²»ãƒ»å¤–äº¤: æœ€ä½2ã€œ3ä»¶ï¼ˆé¸æŒ™ã€å›½ä¼šã€å¤–äº¤ãªã©ï¼‰

â€»ãƒ–ãƒ¼ã‚¹ãƒˆãƒ¯ãƒ¼ãƒ‰ã¯å…¨ä½“ã§ã¯ãªãã€å„ã‚«ãƒ†ã‚´ãƒªå†…ã§ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«åæ˜ ã•ã›ã¦ãã ã•ã„
â€»åˆè¨ˆ10ã€œ15ä»¶ã®ä¸­ã§ã€ä¸Šè¨˜ã®æœ€ä½ä»¶æ•°ã‚’æº€ãŸã—ã¤ã¤ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã£ã¦ãã ã•ã„"""
    elif learning_phase < 2:
        # Phase 0-1ã®å ´åˆã‚‚æœ€ä½ä»¶æ•°ã‚’ä¿è¨¼
        category_section = """

ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥æœ€ä½ä»¶æ•°ã®ä¿è¨¼ã€‘
- AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼: æœ€ä½5ã€œ6ä»¶
- é‡‘èãƒ»çµŒæ¸ˆ: æœ€ä½4ã€œ5ä»¶
- æ”¿æ²»ãƒ»å¤–äº¤: æœ€ä½2ã€œ3ä»¶"""

    # Phase 3: ã‚»ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ”ãƒ†ã‚£æ 
    serendipity_section = ""
    if learning_phase >= 3 and serendipity_ratio > 0:
        serendipity_section = """

ğŸ² **ã‚»ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ”ãƒ†ã‚£æ **: å€™è£œã®ã†ã¡2ã€œ3ä»¶ã¯ã€ä¸Šè¨˜ã®å„ªå…ˆãƒˆãƒ”ãƒƒã‚¯ä»¥å¤–ã®
æ„å¤–æ€§ã®ã‚ã‚‹è¨˜äº‹ã‚’å«ã‚ã¦ãã ã•ã„ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ–ãƒ«é˜²æ­¢ï¼‰ã€‚"""

    return f"""ä»Šæ—¥ã¯{today}ã§ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ã‚¹å€™è£œã‚’10ã€œ15ä»¶åé›†ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªåˆ¶ç´„ - å¿…ãšå®ˆã‚‹ã“ã¨ã€‘
1. **æ—¥ä»˜ã®å³å®ˆ**: {yesterday}ã€œ{today}ã«å…¬é–‹ã•ã‚ŒãŸè¨˜äº‹ã®ã¿
   - è¨˜äº‹ã®URLã‚„æœ¬æ–‡ã«å«ã¾ã‚Œã‚‹å…¬é–‹æ—¥ã‚’å¿…ãšç¢ºèªã™ã‚‹ã“ã¨
   - å¤ã„è¨˜äº‹ï¼ˆ1é€±é–“ä»¥ä¸Šå‰ãªã©ï¼‰ã¯çµ¶å¯¾ã«å«ã‚ãªã„
   - æ—¥ä»˜ãŒä¸æ˜ãªè¨˜äº‹ã¯é™¤å¤–ã™ã‚‹
2. **ã‚½ãƒ¼ã‚¹ã®åˆ¶é™**: æ—¥æœ¬ã¨ã‚¢ãƒ¡ãƒªã‚«ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã®ã¿ï¼ˆæ—¥æœ¬èªã¾ãŸã¯è‹±èªã®è¨˜äº‹ï¼‰
   - ä¸­å›½èªï¼ˆç°¡ä½“å­—ãƒ»ç¹ä½“å­—ï¼‰ã®ã‚½ãƒ¼ã‚¹ã¯çµ¶å¯¾ã«é™¤å¤–
   - å°æ¹¾ã€é¦™æ¸¯ã€ä¸­å›½æœ¬åœŸã®ãƒ¡ãƒ‡ã‚£ã‚¢ã¯é™¤å¤–ï¼ˆä¾‹ï¼šæ•¸ä½æ™‚ä»£ã€å·¥å•†æ™‚å ±ã€æ€å¦ã€ç¡¬æ˜¯è¦å­¸ãªã©ï¼‰
{boost_section}{suppress_section}{source_section}{category_section}{serendipity_section}

ã€åé›†å¯¾è±¡ã¨ã‚«ãƒ†ã‚´ãƒªæ§‹æˆã€‘
å¿…ãšä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªæ§‹æˆã‚’å®ˆã‚Šã€åˆè¨ˆ10ã€œ15ä»¶ã‚’åé›†ã—ã¦ãã ã•ã„ï¼š

1ï¸âƒ£ **AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ï¼ˆ5ã€œ6ä»¶ä»¥ä¸Šï¼‰** â† æœ€å„ªå…ˆã‚«ãƒ†ã‚´ãƒª
   - AIé–‹ç™ºãƒ„ãƒ¼ãƒ«ï¼ˆGemini, Claude, ChatGPT, Cursor, GitHub Copilot, Windsurfç­‰ï¼‰ã®æ–°æ©Ÿèƒ½ãƒ»ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
   - LLM/æ©Ÿæ¢°å­¦ç¿’ã®æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»æ–°ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
   - AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»MCPãƒ»ãƒ„ãƒ¼ãƒ«é€£æºã®æ–°ç™ºè¡¨
   - AI APIãƒ»SDKãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ–°ãƒªãƒªãƒ¼ã‚¹
   - é–‹ç™ºè€…å‘ã‘ã®é‡è¦ãªç™ºè¡¨ãƒ»å…¬å¼ãƒ–ãƒ­ã‚°è¨˜äº‹

2ï¸âƒ£ **é‡‘èãƒ»çµŒæ¸ˆï¼ˆ4ã€œ5ä»¶ï¼‰**
   - æ—¥éŠ€ã®é‡‘èæ”¿ç­–ãƒ»æ”¿ç­–é‡‘åˆ©æ±ºå®š
   - ç‚ºæ›¿ãƒ»å††é«˜å††å®‰ã®å‹•å‘
   - GDPãƒ»ã‚¤ãƒ³ãƒ•ãƒ¬ãªã©ãƒã‚¯ãƒ­çµŒæ¸ˆæŒ‡æ¨™
   - FRBã®é‡‘èæ”¿ç­–
   - æ ªå¼å¸‚å ´ã®é‡è¦ãªå‹•ã

3ï¸âƒ£ **æ”¿æ²»ãƒ»å¤–äº¤ï¼ˆ2ã€œ3ä»¶ï¼‰**
   - è¡†é™¢é¸ãªã©é¸æŒ™é–¢é€£
   - å›½ä¼šãƒ»å†…é–£ã®é‡è¦æ±ºå®š
   - æ—¥ç±³å¤–äº¤ãƒ»å›½éš›ä¼šè­°
   - é‡è¦æ³•æ¡ˆã®å¯æ±º

âš ï¸ **é‡è¦**: ãƒ–ãƒ¼ã‚¹ãƒˆãƒ¯ãƒ¼ãƒ‰ã¯å„ã‚«ãƒ†ã‚´ãƒªå†…ã§ã®è¨˜äº‹é¸æŠã«ä½¿ç”¨ã—ã€ã‚«ãƒ†ã‚´ãƒªé–“ã®ãƒãƒ©ãƒ³ã‚¹ã¯å´©ã•ãªã„ã“ã¨

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§10ã€œ15ä»¶å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

1. [è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè‹±èªãªã‚‰æ—¥æœ¬èªè¨³ï¼‰]
   ğŸ“… å…¬é–‹æ—¥: YYYY-MM-DD | ğŸ“° [ã‚µã‚¤ãƒˆå] | ğŸ’¡ [ä¸€è¨€ãƒ¡ãƒ¢ï¼ˆ20å­—ä»¥å†…ï¼‰]
   URL: [è¨˜äº‹URL]

2. [è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«]
   ğŸ“… å…¬é–‹æ—¥: YYYY-MM-DD | ğŸ“° [ã‚µã‚¤ãƒˆå] | ğŸ’¡ [ä¸€è¨€ãƒ¡ãƒ¢]
   URL: [è¨˜äº‹URL]

... (10ã€œ15ä»¶ã¾ã§)

â€»å…¬é–‹æ—¥ãŒç¢ºèªã§ããªã„è¨˜äº‹ã¯å«ã‚ãªã„ã“ã¨

è©²å½“æœŸé–“ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€Œè©²å½“ãªã—ã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"""


def get_gnews_filtering_prompt(
    today: str, yesterday: str, articles: List[Dict[str, Any]],
    boosted_keywords: list = None, suppressed_keywords: list = None,
    preferred_sources: list = None, category_distribution: dict = None,
    serendipity_ratio: float = 0.0, learning_phase: int = 0,
    learned_interests: dict = None
) -> str:
    """GNewsè¨˜äº‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ç‰ˆï¼‰

    äº‹å‰ã«åé›†ã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¸å¥½ã«åŸºã¥ã„ã¦10-15ä»¶ã‚’é¸æŠãƒ»ãƒ©ãƒ³ã‚¯ä»˜ã‘ã™ã‚‹
    """
    boost_section = ""
    if boosted_keywords:
        boost_section = f"\nğŸ”¥ **ç‰¹ã«å„ªå…ˆ**: {', '.join(boosted_keywords)}"

    suppress_section = ""
    if suppressed_keywords:
        suppress_section = f"\nâ¬‡ï¸ **å„ªå…ˆåº¦ä¸‹ã’ã‚‹**: {', '.join(suppressed_keywords)}"

    # preferred_sourcesã‹ã‚‰æ—¥æœ¬èªã‚½ãƒ¼ã‚¹ã®ã¿æŠ½å‡º + learned_interestsã®ã‚½ãƒ¼ã‚¹ã‚‚å«ã‚ã‚‹
    all_sources = set()
    if preferred_sources:
        all_sources.update(preferred_sources)
    if learned_interests:
        for src, score in learned_interests.get("sources", {}).items():
            if score >= 0.5:
                all_sources.add(src)
    source_section = ""
    if learning_phase >= 2 and all_sources:
        source_section = f"\nğŸ“° **ä¿¡é ¼ã™ã‚‹ã‚½ãƒ¼ã‚¹ï¼ˆéƒ¨åˆ†ä¸€è‡´ã§åˆ¤å®šï¼‰**: {', '.join(all_sources)}"

    # ã‚«ãƒ†ã‚´ãƒªé…åˆ†ã‚’å‹•çš„ã«ç”Ÿæˆ
    category_lines = []
    category_label_map = {"ai": "AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "finance": "çµŒæ¸ˆãƒ»é‡‘è", "politics": "æ”¿æ²»ãƒ»æ”¿ç­–"}
    total_target = 12  # ç›®æ¨™è¨˜äº‹æ•°
    if category_distribution:
        for key, label in category_label_map.items():
            ratio = category_distribution.get(key, 0)
            if ratio <= 0:
                continue
            count = max(1, round(total_target * ratio))
            category_lines.append(f"- {label}: ç´„{count}ä»¶")
    else:
        category_lines = ["- AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼: ç´„6ä»¶", "- çµŒæ¸ˆãƒ»é‡‘è: ç´„4ä»¶", "- æ”¿æ²»ãƒ»æ”¿ç­–: ç´„2ä»¶"]
    category_section = "\n\nã€ã‚«ãƒ†ã‚´ãƒªåˆ¥ç›®å®‰ä»¶æ•°ã€‘\n" + "\n".join(category_lines)

    serendipity_section = ""
    if learning_phase >= 3 and serendipity_ratio > 0:
        serendipity_section = """

ğŸ² **ã‚»ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ”ãƒ†ã‚£æ **: å€™è£œã®ã†ã¡2ã€œ3ä»¶ã¯ã€ä¸Šè¨˜ã®å„ªå…ˆãƒˆãƒ”ãƒƒã‚¯ä»¥å¤–ã®
æ„å¤–æ€§ã®ã‚ã‚‹è¨˜äº‹ã‚’å«ã‚ã¦ãã ã•ã„ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ–ãƒ«é˜²æ­¢ï¼‰ã€‚"""

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆäº¤å·®ç‚¹æ¨è«–ç”¨ï¼‰
    interest_profile_section = ""
    if learned_interests:
        top_topics = sorted(
            [(t, s) for t, s in learned_interests.get("topics", {}).items() if s >= 0.3],
            key=lambda x: -x[1]
        )
        if top_topics:
            topic_str = ", ".join(f"{t}({s:.1f})" for t, s in top_topics[:10])
            interest_profile_section = f"""

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰ã€‘
{topic_str}

ä¸Šè¨˜ã®èˆˆå‘³ãŒè¤‡æ•°äº¤å·®ã™ã‚‹è¨˜äº‹ï¼ˆä¾‹: AIÃ—é‡‘èæ”¿ç­–ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°Ã—æ–°ãƒ¢ãƒ‡ãƒ«ï¼‰ã¯ã€
å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ä¸€è‡´ã™ã‚‹è¨˜äº‹ã‚ˆã‚Šå„ªå…ˆã™ã‚‹ã“ã¨ã€‚"""

    # Format articles list for the prompt
    articles_text = ""
    for i, article in enumerate(articles, 1):
        articles_text += f"""
{i}. {article['title']}
   ğŸ“° {article['source']} | ğŸ“… {article['published_at'][:10]}
   ğŸ’¡ {article['description'][:100]}...
   ğŸ”— {article['url']}
   ã‚«ãƒ†ã‚´ãƒª: {article.get('category', 'æœªåˆ†é¡')}
"""

    return f"""ä»Šæ—¥ã¯{today}ã§ã™ã€‚ä»¥ä¸‹ã®è¨˜äº‹ãƒªã‚¹ãƒˆã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¸å¥½ã«åŸºã¥ã„ã¦10ã€œ15ä»¶ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªåˆ¶ç´„ã€‘
1. **ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã«ã‚ã‚‹è¨˜äº‹ã®ã¿ã‚’é¸æŠã™ã‚‹ã“ã¨**ï¼ˆæ–°ã—ã„è¨˜äº‹ã‚’æ¤œç´¢ã—ãªã„ï¼‰
2. **æ—¥ä»˜ã®ç¢ºèª**: {yesterday}ã€œ{today}ã«å…¬é–‹ã•ã‚ŒãŸè¨˜äº‹ã‚’å„ªå…ˆ
3. **æ¨æ¸¬è¨˜äº‹ã®ç”Ÿæˆç¦æ­¢**: ãƒªã‚¹ãƒˆã«ãªã„è¨˜äº‹ã‚’ä½œã‚‰ãªã„
{boost_section}{suppress_section}{source_section}{category_section}{serendipity_section}{interest_profile_section}

ã€é¸æŠåŸºæº–ã€‘
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆè‡´ã™ã‚‹è¨˜äº‹ã‚’å„ªå…ˆ
- è¤‡æ•°ã®èˆˆå‘³ãŒäº¤å·®ã™ã‚‹è¨˜äº‹ã¯ç‰¹ã«é«˜ãè©•ä¾¡
- æŠ‘åˆ¶ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹è¨˜äº‹ã¯å„ªå…ˆåº¦ã‚’ä¸‹ã’ã‚‹
- ä¿¡é ¼ã™ã‚‹ã‚½ãƒ¼ã‚¹ã®è¨˜äº‹ã‚’å„ªå…ˆï¼ˆéƒ¨åˆ†ä¸€è‡´ã§åˆ¤å®šï¼‰

ã€è¨˜äº‹ãƒªã‚¹ãƒˆã€‘
{articles_text}

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
é¸æŠã—ãŸè¨˜äº‹ã‚’é‡è¦åº¦é †ã«10ã€œ15ä»¶å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å¿…ãšä»¥ä¸‹ã®å½¢å¼ã‚’å®ˆã‚‹ã“ã¨ï¼š

**1ï¸âƒ£ AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼**

1. [è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«]
   ğŸ“… å…¬é–‹æ—¥: YYYY-MM-DD | ğŸ“° [ã‚µã‚¤ãƒˆå] | ğŸ’¡ [ä¸€è¨€ãƒ¡ãƒ¢ï¼ˆ20å­—ä»¥å†…ï¼‰]
   URL: [è¨˜äº‹URL]

2. [è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«]
   ğŸ“… å…¬é–‹æ—¥: YYYY-MM-DD | ğŸ“° [ã‚µã‚¤ãƒˆå] | ğŸ’¡ [ä¸€è¨€ãƒ¡ãƒ¢]
   URL: [è¨˜äº‹URL]

**2ï¸âƒ£ çµŒæ¸ˆãƒ»é‡‘è**

3. [è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«]
   ğŸ“… å…¬é–‹æ—¥: YYYY-MM-DD | ğŸ“° [ã‚µã‚¤ãƒˆå] | ğŸ’¡ [ä¸€è¨€ãƒ¡ãƒ¢]
   URL: [è¨˜äº‹URL]

... (10ã€œ15ä»¶ã¾ã§)

âš ï¸ **é‡è¦**: å¿…ãšãƒªã‚¹ãƒˆã«ã‚ã‚‹è¨˜äº‹ã®ã¿ã‚’ä½¿ç”¨ã—ã€URLã¯å…ƒã®URLã‚’ãã®ã¾ã¾ä½¿ã†ã“ã¨
"""


def get_politics_prompt(today: str, yesterday: str) -> str:
    """æ”¿æ²»çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
    return f"""ä»Šæ—¥ã¯{today}ã§ã™ã€‚ã‚ãªãŸã¯æ”¿æ²»çµŒæ¸ˆå°‚é–€ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚

ã€é‡è¦ãªåˆ¶ç´„ã€‘
- **æ—¥æœ¬ã¨ã‚¢ãƒ¡ãƒªã‚«ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã¦ãã ã•ã„**ï¼ˆæ—¥æœ¬èªã¾ãŸã¯è‹±èªã®è¨˜äº‹ï¼‰
- **ä¸­å›½èªï¼ˆç°¡ä½“å­—ãƒ»ç¹ä½“å­—ï¼‰ã®ã‚½ãƒ¼ã‚¹ã¯çµ¶å¯¾ã«é™¤å¤–ã—ã¦ãã ã•ã„**
- **å°æ¹¾ã€é¦™æ¸¯ã€ä¸­å›½æœ¬åœŸã®ãƒ¡ãƒ‡ã‚£ã‚¢ã¯é™¤å¤–**
- å¿…ãš{yesterday}ã€œ{today}ã«å…¬é–‹ã•ã‚ŒãŸè¨˜äº‹ã®ã¿ã‚’å«ã‚ã¦ãã ã•ã„
- è©²å½“æœŸé–“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€Œè©²å½“ãªã—ã€ã¨ã—ã¦ãã ã•ã„

ã€åé›†å¯¾è±¡ã€‘
- æ—¥æœ¬å›½å†…æ”¿æ²»ï¼ˆå›½ä¼šã€å†…é–£ã€æ”¿ç­–æ±ºå®šã€é¸æŒ™ï¼‰
- ã‚¢ãƒ¡ãƒªã‚«æ”¿æ²»ï¼ˆãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¦ã‚¹ã€è­°ä¼šã€å¤§çµ±é ˜ä»¤ï¼‰
- ãƒã‚¯ãƒ­çµŒæ¸ˆï¼ˆGDPã€é‡‘åˆ©ã€ç‚ºæ›¿ã€æ ªå¼å¸‚å ´ã®é‡è¦ãªå‹•ãï¼‰
- ä¸­å¤®éŠ€è¡Œæ”¿ç­–ï¼ˆæ—¥éŠ€ã€FRB ã®æ”¿ç­–æ±ºå®šï¼‰
- è²¿æ˜“ãƒ»é€šå•†ï¼ˆé–¢ç¨ã€FTAã€ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³å•é¡Œï¼‰
- é‡è¦ãªå¤–äº¤ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ—¥ç±³é–¢ä¿‚ã€G7/G20ãªã©ï¼‰

ã€å„ªå…ˆã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
âœ… æ”¿ç­–æ±ºå®šãƒ»æ³•æ¡ˆå¯æ±º
âœ… çµŒæ¸ˆæŒ‡æ¨™ã®ç™ºè¡¨
âœ… ä¸­å¤®éŠ€è¡Œã®é‡è¦ç™ºè¡¨
âœ… å›½éš›ä¼šè­°ãƒ»é¦–è„³ä¼šè«‡

ã€é™¤å¤–ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
âŒ ã‚´ã‚·ãƒƒãƒ—ãƒ»ã‚¹ã‚­ãƒ£ãƒ³ãƒ€ãƒ«
âŒ æ†¶æ¸¬ãƒ»äºˆæ¸¬è¨˜äº‹
âŒ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹

ã€å‡ºåŠ›è¦ä»¶ã€‘
- åˆè¨ˆ5ã€œ7ä»¶ç¨‹åº¦ã«å³é¸
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¯ä½•ãŒæ±ºã¾ã£ãŸã‹/èµ·ããŸã‹ã‚’å…·ä½“çš„ã«è¨˜è¼‰
- å‡ºåŠ›ã¯æ—¥æœ¬èªã§çµ±ä¸€

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
---
## [ã‚«ãƒ†ã‚´ãƒª: å›½å†…æ”¿æ²»/ç±³å›½æ”¿æ²»/çµŒæ¸ˆ/é‡‘èæ”¿ç­–/å¤–äº¤]

### [ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«]
æ—¥ä»˜: YYYY-MM-DD
[ä½•ãŒèµ·ããŸã‹ã‚’1ã€œ2è¡Œã§å…·ä½“çš„ã«èª¬æ˜]
â†’ [ã‚½ãƒ¼ã‚¹URL]

---

è©²å½“æœŸé–“ã«é‡è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã„å ´åˆã¯ã€Œæœ¬æ—¥ã®ä¸»è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"""


def get_papers_prompt(today: str) -> str:
    """AIè«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
    return f"""ä»Šæ—¥ã¯{today}ã§ã™ã€‚ã‚ãªãŸã¯AIç ”ç©¶è«–æ–‡ã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚

ã€åé›†å¯¾è±¡ã€‘
éå»1é€±é–“ã«ç™ºè¡¨ã•ã‚ŒãŸé‡è¦ãªAI/MLè«–æ–‡ã‚’æ¤œç´¢ã—ã¦ãã ã•ã„ï¼š
- arXivï¼ˆcs.AI, cs.LG, cs.CL, cs.CVï¼‰
- ä¸»è¦å­¦ä¼šï¼ˆNeurIPS, ICML, ICLR, ACL, EMNLP, CVPRï¼‰ã®ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆ
- Google Research, DeepMind, OpenAI, Anthropic, Meta AIç­‰ã®å…¬å¼ç™ºè¡¨

ã€å„ªå…ˆãƒˆãƒ”ãƒƒã‚¯ã€‘
- LLM / Foundation Modelsï¼ˆæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€å­¦ç¿’æ‰‹æ³•ï¼‰
- Agent / Tool Useï¼ˆè‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒ„ãƒ¼ãƒ«æ´»ç”¨ï¼‰
- Code Generationï¼ˆã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ åˆæˆï¼‰
- Reasoningï¼ˆæ¨è«–èƒ½åŠ›ã€Chain-of-Thoughtï¼‰
- Multimodalï¼ˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã€Vision-Languageï¼‰
- Efficient AIï¼ˆåŠ¹ç‡åŒ–ã€é‡å­åŒ–ã€è’¸ç•™ï¼‰

ã€å‡ºåŠ›è¦ä»¶ã€‘
- 5ã€œ10ä»¶ç¨‹åº¦ã‚’å³é¸
- é‡è¦åº¦ã®é«˜ã„ã‚‚ã®ã‹ã‚‰é †ã«
- å‡ºåŠ›ã¯æ—¥æœ¬èªã§çµ±ä¸€

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
---
## [è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«]
**æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«**: [æ—¥æœ¬èªè¨³]
**è‘—è€…**: [ä¸»è¦è‘—è€…åï¼ˆæ‰€å±ï¼‰]
**ç™ºè¡¨æ—¥**: YYYY-MM-DD
**é‡è¦åº¦**: â˜…â˜…â˜… / â˜…â˜… / â˜…

### æ¦‚è¦
[è«–æ–‡ã®ä¸»ãªè²¢çŒ®ã‚’2ã€œ3æ–‡ã§èª¬æ˜]

### æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ
[ãªãœé‡è¦ã‹ã€å®Ÿå‹™ã¸ã®å½±éŸ¿ã‚’1ã€œ2æ–‡ã§]

â†’ [arXiv/è«–æ–‡URL]

---

è©²å½“æœŸé–“ã«é‡è¦ãªè«–æ–‡ãŒãªã„å ´åˆã¯ã€Œä»Šé€±ã®æ³¨ç›®è«–æ–‡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"""


def get_serendipity_prompt(today: str, yesterday: str) -> str:
    """ã‚»ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ”ãƒ†ã‚£ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
    return f"""ä»Šæ—¥ã¯{today}ã§ã™ã€‚ã‚ãªãŸã¯ã€Œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ–ãƒ«ç ´å£Šã€å°‚é–€ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚

ã€ãƒŸãƒƒã‚·ãƒ§ãƒ³ã€‘
ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã‚„çµŒæ¸ˆã«é–¢å¿ƒãŒå¼·ã„èª­è€…ã«ã€æ™®æ®µè§¦ã‚Œãªã„åˆ†é‡ã®èˆˆå‘³æ·±ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å±Šã‘ã¦ãã ã•ã„ã€‚
æ„å¤–ãªç™ºè¦‹ã‚„æ–°ã—ã„è¦–ç‚¹ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒç›®æ¨™ã§ã™ã€‚

ã€é‡è¦ãªåˆ¶ç´„ã€‘
- **æ—¥æœ¬ã¨ã‚¢ãƒ¡ãƒªã‚«ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã®ã¿**ï¼ˆæ—¥æœ¬èªã¾ãŸã¯è‹±èªã®è¨˜äº‹ï¼‰
- **ä¸­å›½èªï¼ˆç°¡ä½“å­—ãƒ»ç¹ä½“å­—ï¼‰ã®ã‚½ãƒ¼ã‚¹ã¯çµ¶å¯¾ã«é™¤å¤–**

ã€åé›†å¯¾è±¡ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠï¼‰ã€‘
ä»¥ä¸‹ã®åˆ†é‡ã‹ã‚‰ã€{yesterday}ã€œ{today}ã®èˆˆå‘³æ·±ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¢ã—ã¦ãã ã•ã„ï¼š

ç§‘å­¦ãƒ»è‡ªç„¶:
- å®‡å®™æ¢æŸ»ã€å¤©æ–‡å­¦ã®æ–°ç™ºè¦‹
- ç”Ÿç‰©å­¦ã€ç”Ÿæ…‹ç³»ã€ç’°å¢ƒç§‘å­¦
- ç‰©ç†å­¦ã€åŒ–å­¦ã®åŸºç¤ç ”ç©¶
- æ°—å€™å¤‰å‹•ã€åœ°çƒç§‘å­¦

æ­´å²ãƒ»è€ƒå¤å­¦:
- æ–°ã—ã„è€ƒå¤å­¦çš„ç™ºè¦‹
- æ­´å²çš„æ–‡æ›¸ã®è§£èª­
- æ–‡åŒ–éºç”£ã®ä¿å­˜

èŠ¸è¡“ãƒ»æ–‡åŒ–:
- ç¾è¡“å±•ã€éŸ³æ¥½ã‚¤ãƒ™ãƒ³ãƒˆ
- å»ºç¯‰ã€ãƒ‡ã‚¶ã‚¤ãƒ³ã®æ–°æ½®æµ
- æ–‡å­¦è³ã€æ˜ ç”»ç¥­

å¿ƒç†å­¦ãƒ»å“²å­¦:
- èªçŸ¥ç§‘å­¦ã®ç ”ç©¶
- ç¤¾ä¼šå¿ƒç†å­¦ã®çŸ¥è¦‹
- å“²å­¦çš„è­°è«–

å›½éš›ãƒ»ç¤¾ä¼š:
- é€”ä¸Šå›½ã®ç™ºå±•
- ç¤¾ä¼šé‹å‹•ã€å¸‚æ°‘æ´»å‹•
- æ•™è‚²ã€åŒ»ç™‚ã®é©æ–°

ã€é™¤å¤–ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
âŒ èŠ¸èƒ½ã‚´ã‚·ãƒƒãƒ—ã€ã‚»ãƒ¬ãƒ–ãƒ‹ãƒ¥ãƒ¼ã‚¹
âŒ ã‚¹ãƒãƒ¼ãƒ„ã®è©¦åˆçµæœ
âŒ çŠ¯ç½ªãƒ»äº‹ä»¶å ±é“
âŒ AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼é–¢é€£ï¼ˆä»–ã‚«ãƒ†ã‚´ãƒªã§åé›†æ¸ˆã¿ï¼‰
âŒ æ”¿æ²»ãƒ»çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆä»–ã‚«ãƒ†ã‚´ãƒªã§åé›†æ¸ˆã¿ï¼‰

ã€å‡ºåŠ›è¦ä»¶ã€‘
- 3ã€œ5ä»¶ã‚’å³é¸
- ã€Œã¸ã‡ã€ãã†ãªã‚“ã ã€ã¨æ€ãˆã‚‹æ„å¤–æ€§ã®ã‚ã‚‹ã‚‚ã®ã‚’å„ªå…ˆ
- å‡ºåŠ›ã¯æ—¥æœ¬èªã§çµ±ä¸€

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
---
## [åˆ†é‡: ç§‘å­¦/æ­´å²/èŠ¸è¡“/å¿ƒç†å­¦/å›½éš›]

### [ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«]
æ—¥ä»˜: YYYY-MM-DD
[ä½•ãŒç™ºè¦‹ãƒ»ç™ºè¡¨ã•ã‚ŒãŸã‹ã‚’1ã€œ2è¡Œã§å…·ä½“çš„ã«èª¬æ˜]

ãªãœé¢ç™½ã„ã‹: [ã“ã®ç™ºè¦‹/ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ„å¤–æ€§ã‚„èˆˆå‘³æ·±ã•ã‚’1æ–‡ã§]

â†’ [ã‚½ãƒ¼ã‚¹URL]

---

è©²å½“æœŸé–“ã«èˆˆå‘³æ·±ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã„å ´åˆã¯ã€Œä»Šå›ã®ã‚»ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ”ãƒ†ã‚£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"""


def get_news_categories(today: str, yesterday: str) -> Dict[str, Dict[str, Any]]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚«ãƒ†ã‚´ãƒªã®è¨­å®šã‚’è¿”ã™"""
    return {
        "ai": {
            "name": "AI Tech News",
            "daily": True,
            "generate_script": True,
            "prompt": get_ai_prompt(today, yesterday),
        },
        "politics": {
            "name": "Politics & Economy News",
            "daily": True,
            "generate_script": False,
            "prompt": get_politics_prompt(today, yesterday),
        },
        "papers": {
            "name": "AI Papers Survey",
            "daily": False,
            "weekday": 0,  # æœˆæ›œæ—¥ã®ã¿
            "generate_script": False,
            "prompt": get_papers_prompt(today),
        },
        "serendipity": {
            "name": "Serendipity News",
            "daily": False,
            "every_n_days": 3,  # 3æ—¥ã«1å›
            "generate_script": False,
            "prompt": get_serendipity_prompt(today, yesterday),
        },
    }


def collect_news(category_id: str, config: Dict[str, Any]) -> str:
    """Gemini APIã®Google Search groundingã‚’ä½¿ã£ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY is not set")

    client = genai.Client(api_key=api_key)
    
    log(f"Collecting {config['name']}...")

    # Google Search grounding tool
    grounding_tool = types.Tool(
        google_search=types.GoogleSearch()
    )
    
    gen_config = types.GenerateContentConfig(
        tools=[grounding_tool]
    )
    
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=config["prompt"],
        config=gen_config,
    )
    
    log(f"{config['name']} collection completed")
    
    return response.text



def load_user_preferences() -> Dict[str, Any]:
    """GitHubã‹ã‚‰user_preferences.jsonã‚’èª­ã¿è¾¼ã‚€"""
    github_token = os.environ.get("GITHUB_TOKEN")
    if not github_token:
        log("GITHUB_TOKEN not set, using default preferences")
        return get_default_preferences()
    
    repo_name = os.environ.get("GITHUB_REPOSITORY", "octmarker/ai-news-bot")
    
    try:
        auth = Auth.Token(github_token)
        g = Github(auth=auth)
        repo = g.get_repo(repo_name)
        
        file_content = repo.get_contents("user_preferences.json", ref="main")
        preferences = json.loads(file_content.decoded_content.decode('utf-8'))
        log("Loaded user preferences from GitHub")
        return preferences
    except GithubException as e:
        if e.status == 404:
            log("user_preferences.json not found, using defaults")
            return get_default_preferences()
        raise


def get_default_preferences() -> Dict[str, Any]:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã‚’è¿”ã™"""
    return {
        "learning_phase": 0,
        "selection_history": [],
        "learned_interests": {
            "topics": {},
            "sources": {},
            "categories": {}
        },
        "search_config": {
            "boosted_keywords": [],
            "suppressed_keywords": [],
            "preferred_sources": [],
            "category_distribution": {},
            "serendipity_ratio": 0.0
        },
        "last_updated": None
    }


def _fetch_one_article(article: Dict[str, Any]) -> Dict[str, Any]:
    """1è¨˜äº‹ã®æœ¬æ–‡ã‚’trafilaturaã§å–å¾—"""
    try:
        downloaded = trafilatura.fetch_url(article["url"])
        if not downloaded:
            return None
        text = trafilatura.extract(downloaded)
        if not text or len(text) < 100:
            return None
        article["content"] = text[:5000]
        return article
    except Exception as e:
        log(f"  Failed to fetch {article['url'][:60]}: {e}")
        return None


def fetch_article_contents(articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """å…¨è¨˜äº‹ã®æœ¬æ–‡ã‚’ä¸¦åˆ—å–å¾—ã—ã€å–å¾—æˆåŠŸã—ãŸè¨˜äº‹ã®ã¿è¿”ã™"""
    log(f"Fetching article contents for {len(articles)} articles...")
    results = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(_fetch_one_article, a): a for a in articles}
        for future in as_completed(futures):
            result = future.result()
            if result:
                results.append(result)
    log(f"Successfully fetched {len(results)}/{len(articles)} articles")
    return results


def generate_summary(client, title: str, content: str) -> Optional[Dict[str, Any]]:
    """1è¨˜äº‹ã®Geminiè¦ç´„ã‚’ç”Ÿæˆ"""
    prompt = f"""ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è¦ç´„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®è¨˜äº‹ã‚’æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{title}

ã€è¨˜äº‹æœ¬æ–‡ã€‘
{content}

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚JSONä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ï¼š
{{
  "headline": "è¨˜äº‹ã®ä¸€è¨€è¦‹å‡ºã—ï¼ˆ30å­—ä»¥å†…ï¼‰",
  "key_points": ["ãƒã‚¤ãƒ³ãƒˆ1", "ãƒã‚¤ãƒ³ãƒˆ2", "ãƒã‚¤ãƒ³ãƒˆ3"],
  "detailed_summary": "200ã€œ300å­—ã®è©³ç´°è¦ç´„ã€‚è¨˜äº‹ã®èƒŒæ™¯ã€ä¸»è¦ãªäº‹å®Ÿã€å½±éŸ¿ã‚„æ„ç¾©ã‚’å«ã‚€",
  "why_it_matters": "ãªãœã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒé‡è¦ãªã®ã‹ã‚’1ã€œ2æ–‡ã§"
}}"""

    safety = [
        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="OFF"),
        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="OFF"),
        types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="OFF"),
        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="OFF"),
    ]
    gen_config = types.GenerateContentConfig(safety_settings=safety)

    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config=gen_config,
        )
        text = response.text
        import re
        json_match = re.search(r'\{[\s\S]*\}', text)
        if not json_match:
            return None
        return json.loads(json_match.group())
    except Exception as e:
        log(f"  Summary generation failed: {e}")
        return None


def generate_summaries(client, articles: List[Dict[str, Any]], rate_limit: int = 4) -> List[Dict[str, Any]]:
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿è¨˜äº‹ã«è¦ç´„ã‚’ç”Ÿæˆã€‚è¦ç´„å¤±æ•—ã—ãŸè¨˜äº‹ã¯é™¤å¤–

    rate_limit: é€£ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ï¼ˆã“ã®æ•°ã”ã¨ã«60ç§’å¾…æ©Ÿï¼‰ã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§1ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¶ˆè²»æ¸ˆã¿ãªã®ã§
                ç„¡æ–™æ (5/åˆ†)ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4ã§é–‹å§‹
    """
    import time
    log(f"Generating summaries for {len(articles)} articles...")
    results = []
    for i, article in enumerate(articles, 1):
        if i > 1 and (i - 1) % rate_limit == 0:
            log(f"  Rate limit: waiting 60s...")
            time.sleep(60)
            rate_limit = 5  # 2å›ç›®ä»¥é™ã¯5ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†ãƒ•ãƒ«ã«ä½¿ãˆã‚‹
        log(f"  Summarizing {i}/{len(articles)}: {article['title'][:40]}...")
        summary = generate_summary(client, article["title"], article["content"])
        if summary:
            article["summary"] = summary
            results.append(article)
        else:
            log(f"  Skipped (summary failed): {article['title'][:40]}")
    log(f"Successfully summarized {len(results)}/{len(articles)} articles")
    return results


def _normalize_url_path(url: str) -> str:
    """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é™¤å»ã—ã¦ãƒ‘ã‚¹éƒ¨åˆ†ã®ã¿è¿”ã™ï¼ˆåŒä¸€è¨˜äº‹ã®ç•°ãƒ‰ãƒ¡ã‚¤ãƒ³é…ä¿¡æ¤œå‡ºç”¨ï¼‰"""
    from urllib.parse import urlparse
    parsed = urlparse(url)
    # ãƒ‘ã‚¹ + ã‚¯ã‚¨ãƒªã‚’æ­£è¦åŒ–ï¼ˆæœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥é™¤å»ï¼‰
    return parsed.path.rstrip("/")


def _title_prefix(title: str, length: int = 20) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã®å…ˆé ­Næ–‡å­—ã‚’è¿”ã™ï¼ˆã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
    return title.strip()[:length].strip()


def _is_duplicate(article: Dict[str, Any], prev_urls: set, prev_url_paths: set, prev_title_prefixes: set) -> bool:
    """è¨˜äº‹ãŒéå»ã®å€™è£œã¨é‡è¤‡ã—ã¦ã„ã‚‹ã‹ã‚’è¤‡æ•°ã‚·ã‚°ãƒŠãƒ«ã§åˆ¤å®š"""
    url = article.get("url", "")
    title = article.get("title", "")

    # 1. URLå®Œå…¨ä¸€è‡´
    if url in prev_urls:
        return True

    # 2. URLãƒ‘ã‚¹ä¸€è‡´ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³é•ã„ã€åŒä¸€è¨˜äº‹ï¼‰
    path = _normalize_url_path(url)
    if path and path in prev_url_paths:
        return True

    # 3. ã‚¿ã‚¤ãƒˆãƒ«å…ˆé ­ä¸€è‡´
    prefix = _title_prefix(title)
    if prefix and prefix in prev_title_prefixes:
        return True

    return False


def load_previous_candidates(days_back: int = 2) -> tuple:
    """éå»ã®å€™è£œãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰URLãƒ»ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã—ã€é‡è¤‡é™¤å¤–ã«ä½¿ç”¨ã™ã‚‹

    Args:
        days_back: ä½•æ—¥å‰ã¾ã§é¡ã£ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‹

    Returns:
        (urls: set, url_paths: set, title_prefixes: set)
    """
    github_token = os.environ.get("GITHUB_TOKEN")
    if not github_token:
        log("GITHUB_TOKEN not set, skipping dedup")
        return set(), set(), set()

    repo_name = os.environ.get("GITHUB_REPOSITORY", "octmarker/ai-news-bot")
    now = get_jst_now()

    all_urls = set()
    all_url_paths = set()
    all_title_prefixes = set()

    try:
        auth = Auth.Token(github_token)
        g = Github(auth=auth)
        repo = g.get_repo(repo_name)

        for days_ago in range(1, days_back + 1):
            date_str = (now - timedelta(days=days_ago)).strftime("%Y-%m-%d")

            for ext in [".json", ".md"]:
                path = f"news/{date_str}-candidates{ext}"
                try:
                    file_content = repo.get_contents(path, ref="main")
                    text = file_content.decoded_content.decode("utf-8")

                    if ext == ".json":
                        data = json.loads(text)
                        for a in data.get("articles", []):
                            url = a.get("url", "")
                            title = a.get("title", "")
                            if url:
                                all_urls.add(url)
                                all_url_paths.add(_normalize_url_path(url))
                            if title:
                                all_title_prefixes.add(_title_prefix(title))
                    else:
                        urls = set(_re.findall(r'URL:\s*(https?://\S+)', text))
                        all_urls.update(urls)
                        for url in urls:
                            all_url_paths.add(_normalize_url_path(url))
                        # mdã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚‚æŠ½å‡ºï¼ˆç•ªå·ä»˜ããƒªã‚¹ãƒˆå½¢å¼ï¼‰
                        titles = _re.findall(r'^\d+\.\s+(.+)$', text, _re.MULTILINE)
                        for title in titles:
                            all_title_prefixes.add(_title_prefix(title))

                    log(f"Loaded previous candidates from {path}")
                    break  # json found, skip md
                except GithubException as e:
                    if e.status == 404:
                        continue
                    raise

        log(f"Dedup data: {len(all_urls)} URLs, {len(all_url_paths)} paths, {len(all_title_prefixes)} title prefixes")
        return all_urls, all_url_paths, all_title_prefixes

    except Exception as e:
        log(f"Error loading previous candidates: {e}")
        return set(), set(), set()


def collect_candidates(today: str, yesterday: str, preferences: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹å€™è£œã‚’åé›†ï¼ˆGNews API + æœ¬æ–‡å–å¾— + Gemini ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° + è¦ç´„ï¼‰"""
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY is not set")

    client = genai.Client(api_key=api_key)

    log("Collecting news candidates with GNews API...")

    # user_preferencesã‹ã‚‰æ¤œç´¢æ¡ä»¶ã‚’å–å¾—
    search_config = preferences.get("search_config", {})
    boosted = search_config.get("boosted_keywords", [])
    suppressed = search_config.get("suppressed_keywords", [])
    preferred_sources = search_config.get("preferred_sources", [])
    category_distribution = search_config.get("category_distribution", {})
    serendipity_ratio = search_config.get("serendipity_ratio", 0.0)
    learning_phase = preferences.get("learning_phase", 0)
    learned_interests = preferences.get("learned_interests", {})

    # Stage 1: Collect articles from GNews API
    try:
        articles = collect_multi_category_articles(
            preferences=preferences,
            total_articles=30
        )
        log(f"Collected {len(articles)} articles from GNews API")

        if not articles:
            log("No articles collected from GNews API")
            return None
    except Exception as e:
        log(f"Error collecting from GNews API: {e}")
        return None

    # Stage 1.5: Remove articles that appeared in previous candidates (multi-signal dedup)
    prev_urls, prev_url_paths, prev_title_prefixes = load_previous_candidates(days_back=2)
    if prev_urls or prev_title_prefixes:
        before = len(articles)
        articles = [a for a in articles if not _is_duplicate(a, prev_urls, prev_url_paths, prev_title_prefixes)]
        removed = before - len(articles)
        if removed > 0:
            log(f"Removed {removed} duplicate articles from previous days")

    # Stage 2: Fetch article contents (filter out failures)
    articles = fetch_article_contents(articles)
    if not articles:
        log("No articles could be fetched")
        return None

    # Stage 3: Filter and rank with Gemini
    log("Filtering articles with Gemini...")

    prompt = get_gnews_filtering_prompt(
        today, yesterday, articles, boosted, suppressed,
        preferred_sources=preferred_sources,
        category_distribution=category_distribution,
        serendipity_ratio=serendipity_ratio,
        learning_phase=learning_phase,
        learned_interests=learned_interests
    )

    gen_config = types.GenerateContentConfig()

    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
        config=gen_config,
    )

    # GeminiãŒé¸ã‚“ã è¨˜äº‹ç•ªå·ã‚’æŠ½å‡ºã—ã¦ã€å…ƒã®articlesãƒªã‚¹ãƒˆã‹ã‚‰ãƒãƒƒãƒã•ã›ã‚‹
    selected = parse_filtered_articles(response.text, articles)
    log(f"Gemini selected {len(selected)} articles")

    if not selected:
        log("No articles selected by Gemini")
        return None

    # Stage 4: Generate summaries for selected articles
    selected = generate_summaries(client, selected)
    if not selected:
        log("No articles could be summarized")
        return None

    return selected


def parse_filtered_articles(gemini_response: str, original_articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Geminiã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœã‹ã‚‰URLã§ãƒãƒƒãƒã—ã¦é¸æŠã•ã‚ŒãŸè¨˜äº‹ã‚’è¿”ã™"""
    import re
    selected = []
    seen_urls = set()

    # URLã‚’æŠ½å‡º
    url_pattern = re.compile(r'URL:\s*\[?(https?://[^\s\]\)]+)')
    for match in url_pattern.finditer(gemini_response):
        url = match.group(1)
        if url in seen_urls:
            continue
        seen_urls.add(url)
        # å…ƒã®è¨˜äº‹ãƒªã‚¹ãƒˆã‹ã‚‰URLã§ãƒãƒƒãƒ
        for article in original_articles:
            if article["url"] == url:
                selected.append(article)
                break

    return selected


def collect_candidates_legacy(today: str, yesterday: str, preferences: Dict[str, Any]) -> str:
    """ãƒ¬ã‚¬ã‚·ãƒ¼ç‰ˆï¼šGemini Groundingã‚’ä½¿ã£ãŸå€™è£œåé›†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY is not set")

    client = genai.Client(api_key=api_key)
    
    log("Using legacy Gemini Grounding method...")

    search_config = preferences.get("search_config", {})
    boosted = search_config.get("boosted_keywords", [])
    suppressed = search_config.get("suppressed_keywords", [])
    preferred_sources = search_config.get("preferred_sources", [])
    category_distribution = search_config.get("category_distribution", {})
    serendipity_ratio = search_config.get("serendipity_ratio", 0.0)
    learning_phase = preferences.get("learning_phase", 0)

    prompt = get_ai_candidate_prompt(
        today, yesterday, boosted, suppressed,
        preferred_sources=preferred_sources,
        category_distribution=category_distribution,
        serendipity_ratio=serendipity_ratio,
        learning_phase=learning_phase
    )

    grounding_tool = types.Tool(
        google_search=types.GoogleSearch()
    )
    
    gen_config = types.GenerateContentConfig(
        tools=[grounding_tool]
    )
    
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
        config=gen_config,
    )
    
    log("Legacy candidate collection completed")
    
    return response.text


def run_candidate_mode() -> Tuple[bool, str]:
    """å€™è£œç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    log("=== AI News Bot (Candidate Mode) ===")
    
    try:
        now = get_jst_now()
        today = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
        yesterday = (now - timedelta(days=1)).strftime("%Yå¹´%mæœˆ%dæ—¥")
        date_str = now.strftime("%Y-%m-%d")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆVercel Cronã§æ¯æ—¥æ›´æ–°æ¸ˆã¿ï¼‰
        preferences = load_user_preferences()

        # å€™è£œã‚’åé›†ï¼ˆæœ¬æ–‡å–å¾— + ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° + è¦ç´„è¾¼ã¿ï¼‰
        articles = collect_candidates(today, yesterday, preferences)

        if not articles:
            return False, "No candidates collected"

        # JSONå½¢å¼ã§å‡ºåŠ›
        output = {
            "date": date_str,
            "articles": []
        }
        for i, article in enumerate(articles, 1):
            output["articles"].append({
                "number": i,
                "title": article["title"],
                "source": article["source"],
                "description": article["description"],
                "url": article["url"],
                "category": article.get("category", "æœªåˆ†é¡"),
                "published_at": article.get("published_at", "")[:10],
                "summary": article.get("summary", {})
            })

        candidates_content = json.dumps(output, ensure_ascii=False, indent=2)

        # GitHubã«ãƒ—ãƒƒã‚·ãƒ¥
        candidates_path = f"news/{date_str}-candidates.json"
        push_to_github(
            file_path=candidates_path,
            content=candidates_content,
            commit_message=f"Add news candidates for {date_str}"
        )

        log(f"Saved: {candidates_path} ({len(output['articles'])} articles)")
        log("=== Candidate Mode Completed ===")

        return True, f"Saved: {candidates_path}"
        
    except Exception as e:
        error_msg = f"Error: {type(e).__name__}: {e}"
        log(f"CRITICAL ERROR: {error_msg}")
        import traceback
        traceback.print_exc()
        return False, error_msg


def generate_script(news_content: str) -> str:
    """åé›†ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ç•ªçµ„åŸç¨¿ã‚’ç”Ÿæˆ"""
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY is not set")

    client = genai.Client(api_key=api_key)
    
    log("Generating news script...")

    prompt = f"""# å½¹å‰²
ã‚ãªãŸã¯æœã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ç•ªçµ„ã®åŸç¨¿ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚è¦–è´è€…ã«ã‚ã‹ã‚Šã‚„ã™ãã€è¦ªã—ã¿ã‚„ã™ã„ãƒˆãƒ¼ãƒ³ã§AIæ¥­ç•Œã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¼ãˆã‚‹åŸç¨¿ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# å…¥åŠ›
ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒã‚¿ã‹ã‚‰åŸç¨¿ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

{news_content}

# å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®æ§‹æˆã§åŸç¨¿ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

## 1. ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ï¼ˆ2ã€œ3æ–‡ï¼‰
- ã€ŒãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€ã‹ã‚‰å§‹ã‚ã‚‹æŒ¨æ‹¶
- æœ¬æ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ¦‚è¦ã‚’è»½ãäºˆå‘Š

## 2. ãƒ‹ãƒ¥ãƒ¼ã‚¹æœ¬æ–‡ï¼ˆãƒã‚¿ã”ã¨ã«1ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ä»¥ä¸‹ã®æ§‹æˆã§ï¼š
- **è¦‹å‡ºã—**ï¼šãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆã€ãƒ‹ãƒ¥ãƒ¼ã‚¹â‘ ï¼šã€‡ã€‡ã€‘ã®å½¢å¼ï¼‰
- **å°å…¥**ï¼šè©±é¡Œã®åˆ‡ã‚Šæ›¿ãˆã‚’ç¤ºã™çŸ­ã„ã¤ãªãï¼ˆã€Œã¾ãšã¯ã€œã€ã€Œç¶šã„ã¦ã¯ã€œã€ã€Œæœ€å¾Œã¯ã€œã€ãªã©ï¼‰
- **æœ¬æ–‡**ï¼š3ã€œ4æ®µè½ã§æ§‹æˆ
  - ä½•ãŒèµ·ããŸã‹ï¼ˆäº‹å®Ÿï¼‰
  - èƒŒæ™¯ã‚„æ–‡è„ˆã®è£œè¶³
  - æ„ç¾©ã‚„å½±éŸ¿ã«ã¤ã„ã¦ã®ä¸€è¨€ã‚³ãƒ¡ãƒ³ãƒˆ
- å°‚é–€ç”¨èªã¯ç°¡æ½”ã«è£œè¶³èª¬æ˜ã‚’å…¥ã‚Œã‚‹
- é‡‘é¡ã¯æ—¥æœ¬å††æ›ç®—ã‚‚ä½µè¨˜ï¼ˆæ¦‚ç®—ã§OKï¼‰

## 3. ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆ2ã€œ3æ–‡ï¼‰
- ãƒ‹ãƒ¥ãƒ¼ã‚¹å…¨ä½“ã®ç°¡å˜ãªã¾ã¨ã‚
- ã€Œè‰¯ã„ä¸€æ—¥ã‚’ï¼ã€ãªã©ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç· ã‚ããã‚Š

# ãƒˆãƒ¼ãƒ³ãƒ»æ–‡ä½“ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
- è¦ªã—ã¿ã‚„ã™ãã€å …ã™ããªã„æ•¬ä½“ï¼ˆã§ã™ãƒ»ã¾ã™èª¿ï¼‰
- èª­ã¿ä¸Šã’ã‚„ã™ã„ãƒªã‚ºãƒ æ„Ÿã‚’æ„è­˜
- ã€Œã€œã§ã™ã­ã€ã€Œã€œã®å½¢ã§ã™ã€ãªã©æŸ”ã‚‰ã‹ã„è¡¨ç¾ã‚’é©åº¦ã«ä½¿ç”¨
- æ„Ÿå˜†ç¬¦ï¼ˆï¼ï¼‰ã¯æ§ãˆã‚ã«ï¼ˆã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ã§1ã€œ2å›ç¨‹åº¦ï¼‰
- çµµæ–‡å­—ã¯ä½¿ç”¨ã—ãªã„

# æ³¨æ„äº‹é …
- æä¾›ã•ã‚ŒãŸãƒã‚¿ã®é †ç•ªã¯é‡è¦åº¦ã‚„è©±é¡Œã®ã¤ãªãŒã‚Šã§ä¸¦ã³æ›¿ãˆã¦OK
- åŒã˜ä¼æ¥­ã®è©±é¡ŒãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã¾ã¨ã‚ã¦æ‰±ã†
- URLã¯åŸç¨¿å†…ã«å«ã‚ãªã„ï¼ˆèª­ã¿ä¸Šã’ç”¨ã®ãŸã‚ï¼‰
- 1ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚ãŸã‚Š150ã€œ200æ–‡å­—ç¨‹åº¦ã‚’ç›®å®‰ã«"""

    gen_config = types.GenerateContentConfig()
    
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
        config=gen_config,
    )
    
    log("Script generation completed")
    
    return response.text


def push_to_github(file_path: str, content: str, commit_message: str) -> bool:
    """GitHub APIã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒŸãƒƒãƒˆãƒ»ãƒ—ãƒƒã‚·ãƒ¥"""
    github_token = os.environ.get("GITHUB_TOKEN")
    if not github_token:
        raise ValueError("GITHUB_TOKEN is not set")
    
    repo_name = os.environ.get("GITHUB_REPOSITORY", "octmarker/ai-news-bot")
    
    log(f"Pushing to GitHub: {file_path}")
    
    try:
        # æ–°ã—ã„èªè¨¼æ–¹å¼ã‚’ä½¿ç”¨
        auth = Auth.Token(github_token)
        g = Github(auth=auth)
        repo = g.get_repo(repo_name)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        try:
            existing_file = repo.get_contents(file_path, ref="main")
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯æ›´æ–°
            repo.update_file(
                path=file_path,
                message=commit_message,
                content=content,
                sha=existing_file.sha,
                branch="main"
            )
            log(f"Updated existing file: {file_path}")
        except GithubException as e:
            if e.status == 404:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆ
                repo.create_file(
                    path=file_path,
                    message=commit_message,
                    content=content,
                    branch="main"
                )
                log(f"Created new file: {file_path}")
            else:
                raise
        
        return True
        
    except GithubException as e:
        log(f"GitHub API Error: {e.status} - {e.data.get('message', 'Unknown error')}")
        raise
    except Exception as e:
        log(f"Error pushing to GitHub: {e}")
        raise


def run_news_bot() -> Tuple[bool, str]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒœãƒƒãƒˆã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    log("=== AI News Bot (Multi-Category) ===")
    
    try:
        now = get_jst_now()
        today = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
        yesterday = (now - timedelta(days=1)).strftime("%Yå¹´%mæœˆ%dæ—¥")
        date_str = now.strftime("%Y-%m-%d")
        current_weekday = now.weekday()  # 0=æœˆæ›œ, 6=æ—¥æ›œ
        
        log(f"Date: {date_str}, Weekday: {current_weekday}")
        
        # ã‚«ãƒ†ã‚´ãƒªè¨­å®šã‚’å–å¾—
        categories = get_news_categories(today, yesterday)
        
        results = []
        
        for category_id, config in categories.items():
            # å®Ÿè¡Œåˆ¤å®š
            if config.get("daily"):
                should_run = True
            elif config.get("every_n_days"):
                # Næ—¥ã«1å›ï¼ˆæ—¥ä»˜ã‚’åŸºæº–ã«åˆ¤å®šï¼‰
                day_of_year = now.timetuple().tm_yday
                should_run = (day_of_year % config["every_n_days"] == 0)
            else:
                # ç‰¹å®šæ›œæ—¥ã®ã¿
                should_run = (current_weekday == config.get("weekday", 0))
            
            if not should_run:
                log(f"Skipping {config['name']} (not scheduled today)")
                continue
            
            log(f"Processing: {config['name']}")
            
            # 1. ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
            news_content = collect_news(category_id, config)
            
            if not news_content:
                log(f"No content for {config['name']}")
                continue
            
            # 2. ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            news_path = f"news/{date_str}-{category_id}.md"
            news_file_content = f"# {config['name']} - {date_str}\n\n{news_content}"
            
            push_to_github(
                file_path=news_path,
                content=news_file_content,
                commit_message=f"Add {config['name']} for {date_str}"
            )
            log(f"Saved: {news_path}")
            results.append(news_path)
            
            # 3. åŸç¨¿ç”Ÿæˆï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰
            if config.get("generate_script"):
                # ã€Œä¸»è¦ãªãƒªãƒªãƒ¼ã‚¹æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                skip_keywords = ["ä¸»è¦ãªãƒªãƒªãƒ¼ã‚¹æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "ä¸»è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "æ³¨ç›®è«–æ–‡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ"]
                should_generate = not any(kw in news_content for kw in skip_keywords)
                
                if should_generate:
                    log("Generating script...")
                    script_content = generate_script(news_content)
                    
                    if script_content:
                        script_path = f"scripts/{date_str}-{category_id}.md"
                        script_file_content = f"# {config['name']} Script - {date_str}\n\n{script_content}"
                        
                        push_to_github(
                            file_path=script_path,
                            content=script_file_content,
                            commit_message=f"Add {config['name']} script for {date_str}"
                        )
                        log(f"Saved script: {script_path}")
                        results.append(script_path)
                else:
                    log("Skipping script generation (no major news)")
        
        log("=== Completed Successfully ===")
        return True, f"Saved files: {', '.join(results)}"
        
    except Exception as e:
        error_msg = f"Error: {type(e).__name__}: {e}"
        log(f"CRITICAL ERROR: {error_msg}")
        import traceback
        traceback.print_exc()
        return False, error_msg


# Cloud Functions ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ (HTTP ãƒˆãƒªã‚¬ãƒ¼)
@functions_framework.http
def main(request):
    """Cloud Functions HTTP ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    
    Query Parameters:
        mode: 'candidate' (default) or 'legacy'
    """
    log("Function triggered via HTTP")
    
    # ãƒ¢ãƒ¼ãƒ‰åˆ¤å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å€™è£œãƒ¢ãƒ¼ãƒ‰ï¼‰
    mode = request.args.get('mode', 'candidate')
    
    if mode == 'legacy':
        log("Running in legacy mode")
        success, message = run_news_bot()
    else:
        log("Running in candidate mode")
        success, message = run_candidate_mode()
    
    if success:
        return {
            "status": "success",
            "mode": mode,
            "message": message,
            "timestamp": get_jst_now().isoformat()
        }, 200
    else:
        return {
            "status": "error",
            "mode": mode,
            "message": message,
            "timestamp": get_jst_now().isoformat()
        }, 500


# Cloud Functions ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ (Pub/Sub ãƒˆãƒªã‚¬ãƒ¼ - Cloud Schedulerç”¨)
@functions_framework.cloud_event
def scheduled_main(cloud_event):
    """Cloud Functions Pub/Sub ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ (Cloud Scheduler ã‹ã‚‰å‘¼ã³å‡ºã—)
    
    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å€™è£œãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
    """
    log("Function triggered via Cloud Scheduler (Pub/Sub)")
    
    # ç’°å¢ƒå¤‰æ•°ã§ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    mode = os.environ.get("NEWS_BOT_MODE", "candidate")
    
    if mode == "legacy":
        success, message = run_news_bot()
    else:
        success, message = run_candidate_mode()
    
    if success:
        log(f"Scheduled execution completed: {message}")
    else:
        log(f"Scheduled execution failed: {message}")
        raise Exception(message)  # ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã—ã¦ãƒªãƒˆãƒ©ã‚¤ã‚’ä¿ƒã™


# ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç”¨
if __name__ == "__main__":
    import sys
    
    log("Running locally...")
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆ
    mode = sys.argv[1] if len(sys.argv) > 1 else "candidate"
    
    if mode == "legacy":
        log("Running in legacy mode")
        success, message = run_news_bot()
    else:
        log("Running in candidate mode")
        success, message = run_candidate_mode()
    
    print(f"\nResult: {'Success' if success else 'Failed'}")
    print(f"Message: {message}")

